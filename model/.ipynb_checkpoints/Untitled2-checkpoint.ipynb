{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 1-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 2-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(8078, 1)\n",
      "(8078, 191)\n",
      "Loading 3-th mini-batch ...\n",
      "max: 5.073649, min: -5.041768\n",
      "(9019, 1)\n",
      "(9019, 191)\n",
      "Loading 4-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 5-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 6-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 7-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 8-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(10032, 1)\n",
      "(10032, 191)\n",
      "Loading 9-th mini-batch ...\n",
      "max: 5.087900, min: -5.041768\n",
      "(9011, 1)\n",
      "(9011, 191)\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from time import sleep\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "test_fold = 1\n",
    "\n",
    "import time as time_simple\n",
    "    \n",
    "import Queue\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "    \n",
    "    \n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Process\n",
    "\n",
    "import multiprocessing\n",
    "from itertools import product\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def poolcontext(*args, **kwargs):\n",
    "    pool = multiprocessing.Pool(*args, **kwargs)\n",
    "    yield pool\n",
    "    pool.terminate()\n",
    "    \n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def softmax(x,axis=1):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "def clustering(model,X,i,idx):\n",
    "    dist = model.fit_transform(X)\n",
    "    prob = softmax(-dist)[:,0]\n",
    "    th = np.median(prob)\n",
    "    label = prob>=th\n",
    "#     return i,idx[label==0],idx[label==1],model.cluster_centers_[0],model.cluster_centers_[1]\n",
    "    return i,idx[label==0],idx[label==1],np.mean(X[label==0,:],0),np.mean(X[label==1,:],0)\n",
    "\n",
    "def clustering_unpack(args):\n",
    "    return clustering(*args)\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "    \n",
    "def int2bin(a,bin_size):\n",
    "    return bin(int(a))[2:].zfill(bin_size)\n",
    "\n",
    "def idx_in_subset(subset,y):\n",
    "    fullset = np.unique(y)\n",
    "    # diffset = np.setdiff1d(fullset,subset)\n",
    "    dic_binary = {e:0 for e in fullset}\n",
    "    for e in subset:\n",
    "        dic_binary[e] = 1\n",
    "    idx = np.array(map(lambda x: dic_binary[x],y))\n",
    "    return idx == 1\n",
    "\n",
    "class Reader(threading.Thread):\n",
    "    \"\"\" This class is designed to automatically feed mini-batches.\n",
    "        The reader constantly monitors the state of the variable 'data_buffer'.\n",
    "        When finding the 'data_buffer' is None, the reader will fill a mini-batch into it.\n",
    "        This is done in the backend, i.e., the reader is in an independent thread.\n",
    "        For users, they only need to call iterate_batch() to get a new mini-batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the super class\n",
    "    def __init__(self, x_train, y_train,ratio_subset = 1., rng_seed=123, batch_size=32,n_sample_per_class=2, flag_shuffle=True,flag_x_float32=True,x_train_clustering=None,kmeans_batch_size=45,flag_minibatch_kmeans=True,tree_depth=10,num_core=10,supp_pos_batch_size=32):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        :param test_fold   : int, for testing\n",
    "        :param valid_folds : list of ints, validation set\n",
    "        :param rng_seed    : random seed to make our code more controllable\n",
    "        :param n_nearby    : number of time stamps to consider\n",
    "        :param batch_size  : mini-batch\n",
    "        :param center_pad  : center pad the mls feature with 0's\n",
    "                             Actually, we should pad with -80.\n",
    "        :param valid_fold  : int, for validation\n",
    "        \"\"\"\n",
    "\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        # Settings\n",
    "        self.rng = numpy.random.RandomState(seed=rng_seed)\n",
    "        self.batch_size = batch_size\n",
    "   \n",
    "\n",
    "        # train_data : {'data', 'label', 'file', 'fold', 'salience', 'sound'}\n",
    "        # train_data['data'] is an ndarray, T x D,\n",
    "        # each row a sample for a certain time-stamp\n",
    "\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.dim_time = len(x_train[0])\n",
    "        self.dim_class_num = len(y_train[0])\n",
    "        self.ratio_subset = ratio_subset\n",
    "        self.flag_shuffle = flag_shuffle\n",
    "        self.n_sample = y_train.shape[0]\n",
    "        self.n_sample_per_class = n_sample_per_class\n",
    "        self.flag_x_float32 = flag_x_float32\n",
    "        self.x_train_clustering = x_train_clustering\n",
    "        self.kmeans_batch_size = kmeans_batch_size\n",
    "        self.flag_minibatch_kmeans = flag_minibatch_kmeans\n",
    "        self.num_core = num_core\n",
    "        self.supp_pos_batch_size = supp_pos_batch_size\n",
    "        self.tree_depth = tree_depth\n",
    "        \n",
    "\n",
    "        # Shuffle the data\n",
    "        # We just need to shuffle 'query_index'\n",
    "        # at each beginning of a new epoch\n",
    "        # 'shuffle_index' is a list indicating\n",
    "        # all the positions in 'query_index'\n",
    "        \n",
    "#         start_time = time_simple.time()\n",
    "        self.shuffle_index = self.gen_idx_by_h_clustering()#range(len(self.x_train))\n",
    "#         self.shuffle_index = self.gen_idx()#range(len(self.x_train))\n",
    "#         end_time = time_simple.time()\n",
    "#         print 'process time: %.3f' % (end_time - start_time)\n",
    "\n",
    "        if self.flag_shuffle:\n",
    "            self.rng.shuffle(self.shuffle_index)\n",
    "\n",
    "        self.index_start = 0\n",
    "\n",
    "        # Initialization\n",
    "        self.running = True\n",
    "        self.data_buffer = None\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        # Start thread\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Overwrite the 'run' method of threading.Thread\n",
    "        \"\"\"\n",
    "        while self.running:\n",
    "            if self.data_buffer is None:\n",
    "                if self.index_start + self.batch_size <= len(self.shuffle_index):\n",
    "                    # This case means we are still in this epoch\n",
    "                    batch_index = self.shuffle_index[self.index_start: self.index_start + self.batch_size]\n",
    "                    self.index_start += self.batch_size\n",
    "\n",
    "                elif self.index_start < len(self.shuffle_index):\n",
    "                    # This case means we've come to the\n",
    "                    # end of this epoch, take all the rest data\n",
    "                    # and shuffle the training data again\n",
    "                    batch_index = self.shuffle_index[self.index_start:]\n",
    "\n",
    "                    # Now, we've finished this epoch\n",
    "                    # let's shuffle it again.\n",
    "                    self.shuffle_index = self.gen_idx()#range(len(self.x_train))\n",
    "                    if self.flag_shuffle:\n",
    "                        self.rng.shuffle(self.shuffle_index)\n",
    "                    self.index_start = 0\n",
    "                    \n",
    "                else:\n",
    "                    # This case means index_start == len(shuffle_index)\n",
    "                    # Thus, we've finished this epoch\n",
    "                    # let's shuffle it again.\n",
    "                    self.shuffle_index = self.gen_idx()#range(len(self.x_train))\n",
    "                    if self.flag_shuffle:\n",
    "                        self.rng.shuffle(self.shuffle_index)\n",
    "                    batch_index = self.shuffle_index[0: self.batch_size]\n",
    "                    self.index_start = self.batch_size\n",
    " \n",
    "#                 data = self.x_train[:len(batch_index)].copy()\n",
    "#                 label = self.y_train[:len(batch_index)].copy()\n",
    "#                 for i in range(len(batch_index)):\n",
    "               \n",
    "\n",
    "#                     data[i] = self.x_train[batch_index[i]] \n",
    "\n",
    "#                     label[i] = self.y_train[batch_index[i]]\n",
    "\n",
    "                batch_index_supp_pos = self.supp_pos_idx(batch_index)\n",
    "                batch_index = np.hstack([batch_index,batch_index_supp_pos[:self.supp_pos_batch_size]])\n",
    "   \n",
    "                data = self.x_train[batch_index] \n",
    "                label = self.y_train[batch_index] \n",
    " \n",
    "\n",
    "                with self.lock:\n",
    "                    self.data_buffer = data, label\n",
    "            sleep(0.0001)\n",
    "\n",
    "    def iterate_batch(self):\n",
    "        while self.data_buffer is None:\n",
    "            sleep(0.0001)\n",
    "\n",
    "        data, label = self.data_buffer\n",
    "        if self.flag_x_float32:\n",
    "            data = numpy.asarray(data, dtype=numpy.float32)\n",
    "#         label = numpy.asarray(label, dtype=numpy.int32)\n",
    "        with self.lock:\n",
    "            self.data_buffer = None\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def close(self):\n",
    "        self.running = False\n",
    "        self.join()\n",
    "        \n",
    "    def change_x_train_clustering(self,x_train_clustering):\n",
    "        self.x_train_clustering = x_train_clustering\n",
    "        self.shuffle_index = self.gen_idx_by_h_clustering()#range(len(self.x_train))\n",
    "        self.index_start = 0\n",
    "        \n",
    "    def change_dataset(self,x_train,y_train,x_train_clustering):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.dim_time = len(x_train[0])\n",
    "        self.dim_class_num = len(y_train[0])\n",
    "        self.n_sample = y_train.shape[0]\n",
    "        self.x_train_clustering = x_train_clustering\n",
    "        self.shuffle_index = self.gen_idx_by_h_clustering()#range(len(self.x_train))\n",
    "        self.index_start = 0\n",
    "        \n",
    "    def gen_idx_by_h_clustering(self):\n",
    "        tree_depth = self.tree_depth\n",
    "        \n",
    "        clustering_model_list = []\n",
    "        for l in range(tree_depth):\n",
    "            clustering_model_list_sub = []\n",
    "            for split in range(2**l):\n",
    "                if self.flag_minibatch_kmeans:  \n",
    "                    clustering_model = MiniBatchKMeans(init='k-means++', n_clusters=2, batch_size=self.kmeans_batch_size,\n",
    "                                  n_init=10, max_no_improvement=10, verbose=0)\n",
    "                else:\n",
    "                    clustering_model = KMeans(init='k-means++', n_clusters=2, n_init=10)\n",
    "                clustering_model_list_sub.append(clustering_model)\n",
    "            clustering_model_list.append(clustering_model_list_sub)\n",
    "            \n",
    "        sample_indices_list = [[np.arange(self.x_train_clustering.shape[0])]]\n",
    "        label_code = np.empty((self.x_train_clustering.shape[0],),dtype='|S%d' % tree_depth)\n",
    "        label_code[:]=''\n",
    "        for l in range(tree_depth):\n",
    " \n",
    "            clustering_model_list_sub = clustering_model_list[l]\n",
    "            sample_indices_list_sub = sample_indices_list[l]\n",
    "            num_core = self.num_core\n",
    "            \n",
    "#             results = []\n",
    "#             for i_split in range(len(clustering_model_list_sub)):\n",
    "# #                 print(len(sample_indices_list_sub[i_split]))\n",
    "#                 result_i = clustering(clustering_model_list_sub[i_split],self.x_train_clustering[sample_indices_list_sub[i_split]],i_split,sample_indices_list_sub[i_split])\n",
    "#                 results.append(result_i)    \n",
    "                \n",
    "            with poolcontext(processes=num_core) as pool:\n",
    "                results = pool.map(clustering_unpack, [(clustering_model_list_sub[i_split],self.x_train_clustering[sample_indices_list_sub[i_split]],i_split,sample_indices_list_sub[i_split]) for i_split in range(len(clustering_model_list_sub))])\n",
    "                \n",
    "            dic = {}\n",
    "            for e in results:\n",
    "                dic[e[0]] = [e[1],e[2],e[3],e[4]]\n",
    "                \n",
    "            center_list_sub = []\n",
    "            sample_indices_list_sub_next = []\n",
    "            \n",
    "            for i_split in range(len(clustering_model_list_sub)):\n",
    "                e = dic[i_split]\n",
    "                sample_indices_list_sub_next.append(e[0])\n",
    "                sample_indices_list_sub_next.append(e[1])\n",
    "                center_list_sub.append(e[2])\n",
    "                center_list_sub.append(e[3])\n",
    "                \n",
    "            sample_indices_list.append(sample_indices_list_sub_next)\n",
    "            \n",
    "            if l == 0:\n",
    "                center_list = [center_list_sub]\n",
    "                code_list_sub = np.arange(2)\n",
    "                code_list = [code_list_sub]\n",
    "            else:\n",
    "                center_list.append(center_list_sub)\n",
    "                \n",
    "                code_list_sub_pre = code_list[l-1]\n",
    "                code_list_sub = np.zeros(len(center_list_sub)).astype(int)\n",
    "                for i_split in range(len(center_list_sub)/4):\n",
    "                    c01,c23 = np.array([center_list_sub[i_split*4],center_list_sub[i_split*4+1]]),np.array([center_list_sub[i_split*4+2],center_list_sub[i_split*4+3]])\n",
    "                    dist = pairwise_distances(c01,c23)\n",
    "                    min_idx = np.unravel_index(np.argmin(dist, axis=None), dist.shape)\n",
    "                    code_list_sub[i_split*4+min_idx[0]] = 1 - code_list_sub_pre[i_split*2]\n",
    "                    code_list_sub[i_split*4+1-min_idx[0]] = code_list_sub_pre[i_split*2]\n",
    "                    code_list_sub[i_split*4+2+min_idx[1]] = 1 - code_list_sub_pre[i_split*2+1]\n",
    "                    code_list_sub[i_split*4+2+1-min_idx[1]] = code_list_sub_pre[i_split*2+1]\n",
    "                code_list.append(code_list_sub)\n",
    "            \n",
    "            cont_len = 0\n",
    "            for i_indices,e_indices in enumerate(sample_indices_list_sub_next):\n",
    "                cont_len += len(e_indices)\n",
    "                label_code[e_indices] = np.core.defchararray.add(label_code[e_indices], str(code_list_sub[i_indices]))\n",
    "        idx_ret = []\n",
    "        for e_int in range(2**tree_depth):\n",
    "            code = int2bin(e_int,tree_depth)\n",
    "            idx = list(np.where(label_code == code)[0])\n",
    "            idx_ret.extend(idx)\n",
    "            \n",
    "        return idx_ret\n",
    "    \n",
    "    def supp_pos_idx(self,indices):\n",
    "        y_train = self.y_train[:,0]\n",
    "        n = len(y_train)\n",
    "        idx_all = np.arange(n)\n",
    "        uniid = np.unique(y_train[indices])\n",
    "        idx_uniid = idx_in_subset(uniid,y_train)\n",
    "        idx_all = idx_all[idx_uniid]\n",
    "        y_train = y_train[idx_uniid]\n",
    "        idx_in_idx = self.gen_idx_fixpk(y_train)\n",
    "        idx_ret = idx_all[idx_in_idx]\n",
    "        \n",
    "        return idx_ret\n",
    "            \n",
    "    def gen_idx_fixpk(self,y_train):\n",
    "        \n",
    "        n = len(y_train)\n",
    " \n",
    "        idx = np.arange(n)\n",
    "        class_set,class_num = np.unique(y_train,return_counts=True)\n",
    "        dic_user2sample_num = {class_set[i]: class_num[i] for i in range(len(class_set))}\n",
    "        dic_user2indices = defaultdict(list)\n",
    "\n",
    "        n_sample_per_class = self.n_sample_per_class\n",
    "  \n",
    "        idx_ret = []\n",
    "        self.rng.shuffle(idx)\n",
    "        for i in range(n):\n",
    "            item = idx[i]\n",
    "            cur_id = y_train[item]\n",
    "            \n",
    "            if dic_user2sample_num[cur_id] == 1:\n",
    "                idx_ret.append(item)\n",
    "            elif dic_user2sample_num[cur_id]<n_sample_per_class:\n",
    "                cur_list = dic_user2indices[cur_id]\n",
    "                if len(cur_list) == dic_user2sample_num[cur_id]-1:\n",
    "                    idx_ret.extend(cur_list)\n",
    "                    idx_ret.append(item)\n",
    "                    dic_user2indices[cur_id] = []\n",
    "                else:\n",
    "                    cur_list.append(item)\n",
    "            else:\n",
    "                cur_list = dic_user2indices[cur_id]\n",
    "                if len(cur_list) == n_sample_per_class-1:\n",
    "                    idx_ret.extend(cur_list)\n",
    "                    idx_ret.append(item)\n",
    "                    dic_user2indices[cur_id] = []\n",
    "                    dic_user2sample_num[cur_id] -= n_sample_per_class\n",
    "                else:\n",
    "                    cur_list.append(item)\n",
    " \n",
    "        return idx_ret\n",
    "        \n",
    "    def gen_idx(self):\n",
    "        \n",
    "        y_train = self.y_train[:,0]\n",
    "        n = len(y_train)\n",
    " \n",
    "        idx = np.arange(n)\n",
    "        class_set,class_num = np.unique(y_train,return_counts=True)\n",
    "        dic_user2sample_num = {class_set[i]: class_num[i] for i in range(len(class_set))}\n",
    "        dic_user2indices = defaultdict(list)\n",
    "\n",
    "        n_sample_per_class = self.n_sample_per_class\n",
    "  \n",
    "        idx_ret = []\n",
    "        self.rng.shuffle(idx)\n",
    "        for i in range(n):\n",
    "            item = idx[i]\n",
    "            cur_id = y_train[item]\n",
    "            \n",
    "            if dic_user2sample_num[cur_id] == 1:\n",
    "                idx_ret.append(item)\n",
    "            elif dic_user2sample_num[cur_id]<n_sample_per_class:\n",
    "                cur_list = dic_user2indices[cur_id]\n",
    "                if len(cur_list) == dic_user2sample_num[cur_id]-1:\n",
    "                    idx_ret.extend(cur_list)\n",
    "                    idx_ret.append(item)\n",
    "                    dic_user2indices[cur_id] = []\n",
    "                else:\n",
    "                    cur_list.append(item)\n",
    "            else:\n",
    "                cur_list = dic_user2indices[cur_id]\n",
    "                if len(cur_list) == n_sample_per_class-1:\n",
    "                    idx_ret.extend(cur_list)\n",
    "                    idx_ret.append(item)\n",
    "                    dic_user2indices[cur_id] = []\n",
    "                    dic_user2sample_num[cur_id] -= n_sample_per_class\n",
    "                else:\n",
    "                    cur_list.append(item)\n",
    " \n",
    "        return idx_ret\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    \"\"\" Let's just write the test function\n",
    "        for reader in the same file\n",
    "    \"\"\"\n",
    "    test_fold = 1\n",
    "    valid_folds = [2]\n",
    "    rng_seed = 123\n",
    "    n = 10000\n",
    "    x_train = np.random.randn(n, 191)\n",
    "    y_train = np.random.randint(0,10,size=(n,1))\n",
    "    x_train_clustering = np.random.randn(n,128)\n",
    "\n",
    "\n",
    "    model = Reader(x_train, y_train,ratio_subset = 1., rng_seed=123, batch_size=32,n_sample_per_class=4, \n",
    "                   flag_shuffle=False,flag_x_float32=False,x_train_clustering=x_train_clustering,kmeans_batch_size=10,\n",
    "                   flag_minibatch_kmeans=True,tree_depth=10,num_core=10,supp_pos_batch_size=16)\n",
    "      \n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Loading %d-th mini-batch ...' % i)\n",
    "        data, label = model.iterate_batch()\n",
    "\n",
    "        print('max: %f, min: %f' % (data.max(), data.min()))\n",
    "        print (label.shape)\n",
    "        print (data.shape)\n",
    "\n",
    "    model.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
