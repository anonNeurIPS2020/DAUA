{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchored_npair_loss_meta_v2(inputs_logit, y_true, hidden, net):\n",
    "    # 控制参数\n",
    "    eps = 1e-10\n",
    "    with_l2reg = True\n",
    "    alpha = 1\n",
    "\n",
    "    # 读取数据\n",
    "    corr_threshold = 0.2\n",
    "    input_unspv, input_labels = hidden, y_true\n",
    "\n",
    "    # 计算input_labels相关的变量\n",
    "    mask = torch.einsum(\"ik,jk->ij\", input_labels, input_labels)\n",
    "    mask_neg = 1. - mask\n",
    "\n",
    "    mask_pos_avg = mask / (torch.sum(mask, dim=1, keepdim=True))\n",
    "    mask_neg_avg = mask_neg / (torch.sum(mask_neg, dim=1, keepdim=True))\n",
    "\n",
    "    sample_weight = torch.sum(mask, dim=1) - 1.\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_pos = torch.sign(sample_weight)\n",
    "\n",
    "    sample_weight = torch.sum(mask_neg, dim=1)\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_neg = torch.sign(sample_weight)\n",
    "\n",
    "    input_unspv = F.normalize(input_unspv, p=2, dim=1)\n",
    "    with torch.no_grad():\n",
    "        input_unspv = input_unspv\n",
    "    similarity_matrix_unspv = torch.einsum(\"ik,jk->ij\", input_unspv, input_unspv)\n",
    "    linear_score_pos = alpha * similarity_matrix_unspv\n",
    "    linear_score_neg = alpha * -similarity_matrix_unspv\n",
    "    prob_target_unspv_pos = F.softmax(linear_score_pos, dim=1)\n",
    "    prob_target_unspv_neg = F.softmax(linear_score_neg, dim=1)\n",
    "\n",
    "    gate_unspv_pos = torch.sign(torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - corr_threshold))\n",
    "    gate_unspv_neg = torch.sign(torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - corr_threshold))\n",
    "\n",
    "    if with_l2reg:\n",
    "        reg = torch.mean(torch.sum(torch.pow(inputs_logit, 2), dim=1)).float()\n",
    "        l2loss = torch.mul(0.25 * 0.002, reg)\n",
    "    else:\n",
    "        l2loss = 0.0\n",
    "\n",
    "    # 计算正样本和负样本与含噪标签的cross entropy loss\n",
    "    similarity_matrix = torch.einsum(\"ik,jk->ij\", inputs_logit, inputs_logit)\n",
    "    prob_pos = F.softmax(similarity_matrix, dim=1)\n",
    "    prob_neg = F.softmax(-similarity_matrix, dim=1)\n",
    "\n",
    "    log_prob_pos = torch.log(prob_pos + eps)\n",
    "    log_prob_neg = torch.log(prob_neg + eps)\n",
    "\n",
    "    ce_loss_pos = -log_prob_pos * mask_pos_avg\n",
    "    ce_loss_neg = -log_prob_neg * mask_neg_avg\n",
    "\n",
    "    unspv_loss_pos = -log_prob_pos * prob_target_unspv_pos\n",
    "    unspv_loss_neg = -log_prob_neg * prob_target_unspv_neg\n",
    "\n",
    "    # 计算正样本的loss\n",
    "    xent_loss_pos_clean = gate_unspv_pos * ce_loss_pos\n",
    "    xent_loss_pos_noise = (1. - gate_unspv_pos) * ce_loss_pos\n",
    "\n",
    "    xent_loss_pos_clean = torch.sum(xent_loss_pos_clean, dim=1)\n",
    "    xent_loss_pos_noise = torch.sum(xent_loss_pos_noise, dim=1)\n",
    "    # 去除非法的正负样本的loss\n",
    "    xent_loss_pos_clean = torch.sum(xent_loss_pos_clean * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_pos_noise = torch.sum(xent_loss_pos_noise * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_pos_list = [xent_loss_pos_clean, xent_loss_pos_noise]\n",
    "\n",
    "    grad_list = []\n",
    "    grad_norm_list = []\n",
    "\n",
    "    listOfVariableTensors = []\n",
    "    listOfVariableTensors.extend([p for p in net.parameters()])\n",
    "\n",
    "    for r in range(len(xent_loss_pos_list)):\n",
    "        grad_r = torch.autograd.grad(xent_loss_pos_list[r], listOfVariableTensors, retain_graph=True,allow_unused=True)\n",
    "        grad_norm = get_grad_norm(grad_r)\n",
    "        grad_list.append(grad_r)\n",
    "        grad_norm_list.append(grad_norm)\n",
    "\n",
    "    grad_prod = get_grad_prod(grad_list[0], grad_list[1])\n",
    "    loss_grad_match = - grad_prod / (grad_norm_list[0] * grad_norm_list[1])\n",
    "\n",
    "    xent_loss_pos = xent_loss_pos_clean + loss_grad_match * 0.001\n",
    "\n",
    "    ## 计算负样本的loss\n",
    "    xent_loss_neg_clean = gate_unspv_neg * ce_loss_neg\n",
    "    xent_loss_neg_noise = (1. - gate_unspv_neg) * ce_loss_neg\n",
    "    xent_loss_neg_clean = torch.sum(xent_loss_neg_clean, dim=1)\n",
    "    xent_loss_neg_noise = torch.sum(xent_loss_neg_noise, dim=1)\n",
    "    xent_loss_neg_clean = torch.sum(xent_loss_neg_clean * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss_neg_noise = torch.sum(xent_loss_neg_noise * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss_neg_list = [xent_loss_neg_clean, xent_loss_neg_noise]\n",
    "\n",
    "    grad_list = []\n",
    "    grad_norm_list = []\n",
    "\n",
    "    listOfVariableTensors = []\n",
    "    listOfVariableTensors.extend([p for p in net.parameters()])\n",
    "\n",
    "    for r in range(len(xent_loss_neg_list)):\n",
    "        grad_r = torch.autograd.grad(xent_loss_neg_list[r], listOfVariableTensors, retain_graph=True,allow_unused=True)\n",
    "        grad_norm = get_grad_norm(grad_r)\n",
    "        grad_list.append(grad_r)\n",
    "        grad_norm_list.append(grad_norm)\n",
    "\n",
    "    grad_prod = get_grad_prod(grad_list[0], grad_list[1])\n",
    "    loss_grad_match = - grad_prod / (grad_norm_list[0] * grad_norm_list[1])\n",
    "\n",
    "    xent_loss_neg = xent_loss_pos_clean + loss_grad_match * 0.001\n",
    "\n",
    "    unspv_loss_neg = torch.sum(unspv_loss_neg, dim=1)\n",
    "    unspv_loss_pos = torch.sum(unspv_loss_pos, dim=1)\n",
    "    unspv_loss_neg = torch.sum(unspv_loss_neg * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    unspv_loss_pos = torch.sum(unspv_loss_pos * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_neg_all = xent_loss_neg + unspv_loss_neg\n",
    "    xent_loss_pos_all = xent_loss_pos + unspv_loss_pos\n",
    "    xent_loss = xent_loss_neg_all + xent_loss_pos_all\n",
    "\n",
    "    loss = (l2loss + xent_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchored_npair_loss_v2(inputs_logit, y_true, hidden):\n",
    "    # 控制参数\n",
    "    eps = 1e-10\n",
    "    with_l2reg = True\n",
    "    alpha = 1\n",
    "\n",
    "    # 读取数据\n",
    "    corr_threshold = 0.2\n",
    "    input_unspv, input_labels = hidden, y_true\n",
    "\n",
    "    # 计算input_labels相关的变量\n",
    "    mask = torch.einsum(\"ik,jk->ij\", input_labels, input_labels)\n",
    "    mask_neg = 1. - mask\n",
    "\n",
    "    mask_pos_avg = mask / (torch.sum(mask, dim=1, keepdim=True))\n",
    "    mask_neg_avg = mask_neg / (torch.sum(mask_neg, dim=1, keepdim=True))\n",
    "\n",
    "    sample_weight = torch.sum(mask, dim=1) - 1.\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_pos = torch.sign(sample_weight)\n",
    "\n",
    "    sample_weight = torch.sum(mask_neg, dim=1)\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_neg = torch.sign(sample_weight)\n",
    "\n",
    "    input_unspv = F.normalize(input_unspv, p=2, dim=1)\n",
    "    with torch.no_grad():\n",
    "        input_unspv = input_unspv\n",
    "    similarity_matrix_unspv = torch.einsum(\"ik,jk->ij\", input_unspv, input_unspv)\n",
    "    linear_score_pos = alpha * similarity_matrix_unspv\n",
    "    linear_score_neg = alpha * -similarity_matrix_unspv\n",
    "    prob_target_unspv_pos = F.softmax(linear_score_pos, dim=1)\n",
    "    prob_target_unspv_neg = F.softmax(linear_score_neg, dim=1)\n",
    "    gate_unspv_pos = torch.sign(torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - corr_threshold))\n",
    "    gate_unspv_neg = torch.sign(torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - corr_threshold))\n",
    "\n",
    "    if with_l2reg:\n",
    "        reg = torch.mean(torch.sum(torch.pow(inputs_logit, 2), dim=1)).float()\n",
    "        l2loss = torch.mul(0.25 * 0.002, reg)\n",
    "    else:\n",
    "        l2loss = 0.0\n",
    "\n",
    "    # 计算正样本和负样本与含噪标签的cross entropy loss\n",
    "    similarity_matrix = torch.einsum(\"ik,jk->ij\", inputs_logit, inputs_logit)\n",
    "    prob_pos = F.softmax(similarity_matrix, dim=1)\n",
    "    prob_neg = F.softmax(-similarity_matrix, dim=1)\n",
    "\n",
    "    log_prob_pos = torch.log(prob_pos + eps)\n",
    "    log_prob_neg = torch.log(prob_neg + eps)\n",
    "\n",
    "    ce_loss_pos = -log_prob_pos * mask_pos_avg\n",
    "    ce_loss_neg = -log_prob_neg * mask_neg_avg\n",
    "\n",
    "    unspv_loss_pos = -log_prob_pos * prob_target_unspv_pos\n",
    "    unspv_loss_neg = -log_prob_neg * prob_target_unspv_neg\n",
    "\n",
    "    # 计算最终loss\n",
    "    xent_loss_pos = gate_unspv_pos * ce_loss_pos + (1. - gate_unspv_pos) * unspv_loss_pos\n",
    "    xent_loss_neg = gate_unspv_neg * ce_loss_neg + (1. - gate_unspv_neg) * unspv_loss_neg\n",
    "    xent_loss_pos = torch.sum(xent_loss_pos, dim=1)\n",
    "    xent_loss_neg = torch.sum(xent_loss_neg, dim=1)\n",
    "\n",
    "    # 去除非法的正负样本的loss\n",
    "    xent_loss_pos = torch.sum(xent_loss_pos * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_neg = torch.sum(xent_loss_neg * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss = xent_loss_pos + xent_loss_neg\n",
    "\n",
    "    # 计算l2loss\n",
    "\n",
    "    loss = (l2loss + xent_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*4*4*(1+2)*6/(24*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchored_npair_loss_v3(inputs_logit, y_true, hidden):\n",
    "    # 控制参数\n",
    "    eps = 1e-10\n",
    "    with_l2reg = True\n",
    "    alpha = 1\n",
    "\n",
    "    # 读取数据\n",
    "    corr_threshold = 0.2\n",
    "    input_unspv, input_labels = hidden, y_true\n",
    "\n",
    "    # 计算input_labels相关的变量\n",
    "    mask = torch.einsum(\"ik,jk->ij\", input_labels, input_labels)\n",
    "    mask_neg = 1. - mask\n",
    "\n",
    "    mask_pos_avg = mask / (torch.sum(mask, dim=1, keepdim=True))\n",
    "    mask_neg_avg = mask_neg / (torch.sum(mask_neg, dim=1, keepdim=True))\n",
    "\n",
    "    sample_weight = torch.sum(mask, dim=1) - 1.\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_pos = torch.sign(sample_weight)\n",
    "\n",
    "    sample_weight = torch.sum(mask_neg, dim=1)\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_neg = torch.sign(sample_weight)\n",
    "\n",
    "    input_unspv = F.normalize(input_unspv, p=2, dim=1)\n",
    "    with torch.no_grad():\n",
    "        input_unspv = input_unspv\n",
    "    similarity_matrix_unspv = torch.einsum(\"ik,jk->ij\", input_unspv, input_unspv)\n",
    "    linear_score_pos = alpha * similarity_matrix_unspv\n",
    "    linear_score_neg = alpha * -similarity_matrix_unspv\n",
    "    prob_target_unspv_pos = F.softmax(linear_score_pos, dim=1)\n",
    "    prob_target_unspv_neg = F.softmax(linear_score_neg, dim=1)\n",
    "    gate_unspv_pos = torch.sign(torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - corr_threshold))\n",
    "    gate_unspv_neg = torch.sign(torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - corr_threshold))\n",
    "\n",
    "    if with_l2reg:\n",
    "        reg = torch.mean(torch.sum(torch.pow(inputs_logit, 2), dim=1)).float()\n",
    "        l2loss = torch.mul(0.25 * 0.002, reg)\n",
    "    else:\n",
    "        l2loss = 0.0\n",
    "\n",
    "    # 计算正样本和负样本与含噪标签的cross entropy loss\n",
    "    similarity_matrix = torch.einsum(\"ik,jk->ij\", inputs_logit, inputs_logit)\n",
    "    prob_pos = F.softmax(similarity_matrix, dim=1)\n",
    "    prob_neg = F.softmax(-similarity_matrix, dim=1)\n",
    "\n",
    "    log_prob_pos = torch.log(prob_pos + eps)\n",
    "    log_prob_neg = torch.log(prob_neg + eps)\n",
    "\n",
    "    ce_loss_pos = -log_prob_pos * mask_pos_avg\n",
    "    ce_loss_neg = -log_prob_neg * mask_neg_avg\n",
    "\n",
    "    unspv_loss_pos = -log_prob_pos * prob_target_unspv_pos\n",
    "    unspv_loss_neg = -log_prob_neg * prob_target_unspv_neg\n",
    "\n",
    "    # 计算最终loss\n",
    "    xent_loss_pos = gate_unspv_pos * ce_loss_pos + unspv_loss_pos\n",
    "    xent_loss_neg = gate_unspv_neg * ce_loss_neg + unspv_loss_neg\n",
    "    xent_loss_pos = torch.sum(xent_loss_pos, dim=1)\n",
    "    xent_loss_neg = torch.sum(xent_loss_neg, dim=1)\n",
    "\n",
    "    # 去除非法的正负样本的loss\n",
    "    xent_loss_pos = torch.sum(xent_loss_pos * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_neg = torch.sum(xent_loss_neg * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss = xent_loss_pos + xent_loss_neg\n",
    "\n",
    "    # 计算l2loss\n",
    "\n",
    "    loss = (l2loss + xent_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchored_npair_loss_meta_2_tmp_step(inputs_logit, y_true, hidden, net):\n",
    "    # 控制参数\n",
    "    eps = 1e-10\n",
    "    with_l2reg = True\n",
    "    alpha = 1\n",
    "\n",
    "    # 读取数据\n",
    "    corr_threshold = 0.2\n",
    "    input_unspv, input_labels = hidden, y_true\n",
    "\n",
    "    # 计算input_labels相关的变量\n",
    "    mask = torch.einsum(\"ik,jk->ij\", input_labels, input_labels)\n",
    "    mask_neg = 1. - mask\n",
    "\n",
    "    mask_pos_avg = mask / (torch.sum(mask, dim=1, keepdim=True))\n",
    "    mask_neg_avg = mask_neg / (torch.sum(mask_neg, dim=1, keepdim=True))\n",
    "\n",
    "    sample_weight = torch.sum(mask, dim=1) - 1.\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_pos = torch.sign(sample_weight)\n",
    "\n",
    "    sample_weight = torch.sum(mask_neg, dim=1)\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight_neg = torch.sign(sample_weight)\n",
    "\n",
    "    input_unspv = F.normalize(input_unspv, p=2, dim=1)\n",
    "    with torch.no_grad():\n",
    "        input_unspv = input_unspv\n",
    "    similarity_matrix_unspv = torch.einsum(\"ik,jk->ij\", input_unspv, input_unspv)\n",
    "    linear_score_pos = alpha * similarity_matrix_unspv\n",
    "    linear_score_neg = alpha * -similarity_matrix_unspv\n",
    "    prob_target_unspv_pos = F.softmax(linear_score_pos, dim=1)\n",
    "    prob_target_unspv_neg = F.softmax(linear_score_neg, dim=1)\n",
    "\n",
    "    gate_unspv_pos = torch.sign(torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - corr_threshold))\n",
    "    gate_unspv_neg = torch.sign(torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - corr_threshold))\n",
    "    gate_unspv_pos_label_correction = torch.sign(torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - 1. + corr_threshold))\n",
    "    gate_unspv_neg_label_correction = torch.sign(torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - 1. + corr_threshold))\n",
    "    \n",
    "    if with_l2reg:\n",
    "        reg = torch.mean(torch.sum(torch.pow(inputs_logit, 2), dim=1)).float()\n",
    "        l2loss = torch.mul(0.25 * 0.002, reg)\n",
    "    else:\n",
    "        l2loss = 0.0\n",
    "\n",
    "    # 计算正样本和负样本与含噪标签的cross entropy loss\n",
    "    similarity_matrix = torch.einsum(\"ik,jk->ij\", inputs_logit, inputs_logit)\n",
    "    prob_pos = F.softmax(similarity_matrix, dim=1)\n",
    "    prob_neg = F.softmax(-similarity_matrix, dim=1)\n",
    "\n",
    "    log_prob_pos = torch.log(prob_pos + eps)\n",
    "    log_prob_neg = torch.log(prob_neg + eps)\n",
    "\n",
    "    ce_loss_pos = -log_prob_pos * mask_pos_avg\n",
    "    ce_loss_neg = -log_prob_neg * mask_neg_avg\n",
    "    ce_loss_pos_label_correction = -log_prob_pos * mask_neg_avg\n",
    "    ce_loss_neg_label_correction = -log_prob_neg * mask_pos_avg\n",
    "\n",
    "    unspv_loss_pos = -log_prob_pos * prob_target_unspv_pos\n",
    "    unspv_loss_neg = -log_prob_neg * prob_target_unspv_neg\n",
    "\n",
    "    # 计算正样本的loss\n",
    "    xent_loss_pos_clean = gate_unspv_pos * ce_loss_pos\n",
    "    xent_loss_pos_noise = (1. - gate_unspv_pos) * ce_loss_pos\n",
    "    xent_loss_pos_label_correction = gate_unspv_pos_label_correction * ce_loss_pos_label_correction\n",
    "\n",
    "    xent_loss_pos_clean = torch.sum(xent_loss_pos_clean, dim=1)\n",
    "    xent_loss_pos_noise = torch.sum(xent_loss_pos_noise, dim=1)\n",
    "    xent_loss_pos_label_correction = torch.sum(xent_loss_pos_label_correction, dim=1)\n",
    "    # 去除非法的正负样本的loss\n",
    "    xent_loss_pos_clean = torch.sum(xent_loss_pos_clean * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_pos_noise = torch.sum(xent_loss_pos_noise * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_pos_label_correction = torch.sum(xent_loss_pos_label_correction * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss_pos_list = [xent_loss_pos_clean, xent_loss_pos_noise,xent_loss_pos_label_correction]\n",
    "\n",
    "    grad_list = []\n",
    "    grad_norm_list = []\n",
    "\n",
    "    listOfVariableTensors = []\n",
    "    listOfVariableTensors.extend([p for p in net.parameters()])\n",
    "\n",
    "    for r in range(len(xent_loss_pos_list)):\n",
    "        grad_r = torch.autograd.grad(xent_loss_pos_list[r], listOfVariableTensors, retain_graph=True,allow_unused=True)\n",
    "        grad_norm = get_grad_norm(grad_r)\n",
    "        grad_list.append(grad_r)\n",
    "        grad_norm_list.append(grad_norm)\n",
    "\n",
    "    loss_grad_match = 0.\n",
    "    for r in [1,2]:    \n",
    "        grad_prod = get_grad_prod(grad_list[0], grad_list[r])\n",
    "        loss_grad_match += - grad_prod / (grad_norm_list[0] * grad_norm_list[r])\n",
    "\n",
    "    xent_loss_pos = xent_loss_pos_clean + loss_grad_match * 0.001\n",
    "\n",
    "    ## 计算负样本的loss\n",
    "    xent_loss_neg_clean = gate_unspv_neg * ce_loss_neg\n",
    "    xent_loss_neg_noise = (1. - gate_unspv_neg) * ce_loss_neg\n",
    "    xent_loss_neg_label_correction = gate_unspv_neg_label_correction * ce_loss_neg_label_correction\n",
    "    xent_loss_neg_clean = torch.sum(xent_loss_neg_clean, dim=1)\n",
    "    xent_loss_neg_noise = torch.sum(xent_loss_neg_noise, dim=1)\n",
    "    xent_loss_neg_label_correction = torch.sum(xent_loss_neg_label_correction, dim=1)\n",
    "    xent_loss_neg_clean = torch.sum(xent_loss_neg_clean * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss_neg_noise = torch.sum(xent_loss_neg_noise * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    xent_loss_neg_label_correction = torch.sum(xent_loss_neg_label_correction * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_neg_list = [xent_loss_neg_clean, xent_loss_neg_noise,xent_loss_neg_label_correction]\n",
    "\n",
    "    grad_list = []\n",
    "    grad_norm_list = []\n",
    "\n",
    "    listOfVariableTensors = []\n",
    "    listOfVariableTensors.extend([p for p in net.parameters()])\n",
    "\n",
    "    for r in range(len(xent_loss_neg_list)):\n",
    "        grad_r = torch.autograd.grad(xent_loss_neg_list[r], listOfVariableTensors, retain_graph=True,allow_unused=True)\n",
    "        grad_norm = get_grad_norm(grad_r)\n",
    "        grad_list.append(grad_r)\n",
    "        grad_norm_list.append(grad_norm)\n",
    "\n",
    "    loss_grad_match = 0.\n",
    "    for r in [1,2]:    \n",
    "        grad_prod = get_grad_prod(grad_list[0], grad_list[r])\n",
    "        loss_grad_match += - grad_prod / (grad_norm_list[0] * grad_norm_list[r])\n",
    "\n",
    "    xent_loss_neg = xent_loss_neg_clean + loss_grad_match * 0.001\n",
    "\n",
    "    unspv_loss_neg = torch.sum(unspv_loss_neg, dim=1)\n",
    "    unspv_loss_pos = torch.sum(unspv_loss_pos, dim=1)\n",
    "    unspv_loss_neg = torch.sum(unspv_loss_neg * sample_weight_neg) / torch.sum(sample_weight_neg)\n",
    "    unspv_loss_pos = torch.sum(unspv_loss_pos * sample_weight_pos) / torch.sum(sample_weight_pos)\n",
    "    xent_loss_neg_all = xent_loss_neg + unspv_loss_neg\n",
    "    xent_loss_pos_all = xent_loss_pos + unspv_loss_pos\n",
    "    xent_loss = xent_loss_neg_all + xent_loss_pos_all\n",
    "\n",
    "    loss = (l2loss + xent_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def stat_sim_mat_v2(v):\n",
    "    v_min,v_max,v_mean,v_median =np.min(v),np.max(v),np.mean(v),np.median(v)\n",
    " \n",
    "    p= np.array([2,5,10,20,30,40,50,60,70,80,90,95,98])\n",
    "    v_percentile = np.percentile(v, p)\n",
    "  \n",
    "    return v_min,v_max,v_mean,v_median,p,v_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_sim_mat(v):\n",
    "    v = np.round(10.*v)\n",
    "    uni,count = np.unique(v,return_counts=True)\n",
    "    return uni,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,  -0.,\n",
       "          1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]),\n",
       " array([2486, 5023, 4963, 4992, 5063, 4883, 5059, 4973, 5043, 4977, 5076,\n",
       "        4966, 4884, 5033, 4970, 5044, 5043, 5021, 5008, 4994, 2499]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.random.rand(100000)\n",
    "v = 2.*v - 1.\n",
    "stat_sim_mat(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(10.*(-0.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(10.*(-0.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9999843372547392,\n",
       " 0.9999855061806806,\n",
       " 0.0006476926796734654,\n",
       " 0.00041907590330292344,\n",
       " array([ 2,  5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 98]),\n",
       " array([-9.59512230e-01, -9.00404185e-01, -7.99478725e-01, -5.99975564e-01,\n",
       "        -3.98982849e-01, -1.98410591e-01,  4.19075903e-04,  2.00990077e-01,\n",
       "         4.02902092e-01,  6.01259393e-01,  8.00619520e-01,  9.00633024e-01,\n",
       "         9.60487017e-01]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sim_mat_v2(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7680000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*128*60000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51659246, 0.49376934, 0.3792147 , 0.44338098, 0.60585649,\n",
       "       0.55737313, 0.5348576 , 0.53472207, 0.40176676, 0.50525578,\n",
       "       0.58404559, 0.47501527, 0.47971671, 0.4993767 , 0.40948152,\n",
       "       0.53372301, 0.55832021, 0.51466041, 0.48120701, 0.52186655,\n",
       "       0.53313782, 0.50681102, 0.51702175, 0.43891825, 0.50798977,\n",
       "       0.37407591, 0.36704216, 0.64532432, 0.36081241, 0.51436255,\n",
       "       0.47910773, 0.57868547, 0.4169693 , 0.4900508 , 0.46897939,\n",
       "       0.50849507, 0.53130475, 0.33823885, 0.42791037, 0.50275684,\n",
       "       0.48166469, 0.43064536, 0.56555076, 0.61207521, 0.4913266 ,\n",
       "       0.50763468, 0.40263366, 0.50088329, 0.52251777, 0.54730101,\n",
       "       0.47770008, 0.47790514, 0.51243745, 0.53064291, 0.55369189,\n",
       "       0.48523546, 0.42491078, 0.56233045, 0.5209299 , 0.47378245,\n",
       "       0.55242614, 0.36478064, 0.54527767, 0.39869996, 0.45112624,\n",
       "       0.61577146, 0.53706447, 0.45521491, 0.44147735, 0.55630267,\n",
       "       0.58552156, 0.53391271, 0.55576919, 0.50212352, 0.47082096,\n",
       "       0.60983054, 0.46293622, 0.57146804, 0.42672989, 0.47905648,\n",
       "       0.4933225 , 0.48187976, 0.52401553, 0.47384321, 0.54771954,\n",
       "       0.53323395, 0.47665375, 0.54195478, 0.45681038, 0.47026144,\n",
       "       0.49889276, 0.49758762, 0.52800457, 0.51210151, 0.48114186,\n",
       "       0.57879714, 0.4500787 , 0.47642155, 0.4844951 , 0.53033989])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.beta(32,32,size=(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss_sub(mask,log_prob,bias=0.):\n",
    "    \n",
    "    sample_weight = torch.sum(mask, dim=1) + float(bias)\n",
    "    sample_weight = torch.max(0.0 * sample_weight, sample_weight)\n",
    "    sample_weight = torch.sign(sample_weight)\n",
    "    \n",
    "    mask_avg = mask / (torch.sum(mask, dim=1, keepdim=True)+1e-6)\n",
    "    \n",
    "    loss = - log_prob * mask_avg\n",
    "    loss = torch.sum(loss, dim=1)\n",
    "    loss = torch.sum(loss * sample_weight) / (torch.sum(sample_weight)+1e-6)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def gen_loss_sub_unspv(log_prob,prob_target):\n",
    "     \n",
    "    loss = - log_prob * prob_target\n",
    "    loss = torch.sum(loss, dim=1)\n",
    "    loss = torch.mean(loss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def gen_meta_loss(xent_loss_pos_list,net):\n",
    "    \n",
    "    grad_list = []\n",
    "    grad_norm_list = []\n",
    "\n",
    "    listOfVariableTensors = []\n",
    "    listOfVariableTensors.extend([p for p in net.parameters()])\n",
    "\n",
    "    for r in range(len(xent_loss_pos_list)):\n",
    "        grad_r = torch.autograd.grad(xent_loss_pos_list[r], listOfVariableTensors, retain_graph=True, allow_unused=True)\n",
    "        grad_norm = get_grad_norm(grad_r)\n",
    "        grad_list.append(grad_r)\n",
    "        grad_norm_list.append(grad_norm)\n",
    "\n",
    "    loss_grad_match = 0.\n",
    "    for r in [1,2]:\n",
    "        with torch.no_grad():\n",
    "            grad_list_0 = grad_list[0]\n",
    "            grad_norm_list_0 = grad_norm_list[0]\n",
    "        grad_prod = get_grad_prod(grad_list_0, grad_list[r])\n",
    "        loss_grad_match += - grad_prod / (grad_norm_list_0 * grad_norm_list[r])\n",
    "        \n",
    "    return loss_grad_match\n",
    "    \n",
    "    \n",
    "def anchored_npair_loss_meta_2_tmp_step_fix_clean(inputs_logit, y_true, hidden, net):\n",
    "    # 控制参数\n",
    "    eps = 1e-10\n",
    "    with_l2reg = True\n",
    "    alpha = 1\n",
    "\n",
    "    # 读取数据\n",
    "    corr_threshold = 0.75454967\n",
    "    input_unspv, input_labels = hidden, y_true\n",
    "\n",
    "    # 计算input_labels相关的变量\n",
    "    mask = torch.einsum(\"ik,jk->ij\", input_labels, input_labels)\n",
    "    mask_neg = 1. - mask\n",
    " \n",
    "    # 计算无监督相关性，以及prob\n",
    "    input_unspv = F.normalize(input_unspv, p=2, dim=1)\n",
    "    with torch.no_grad():\n",
    "        input_unspv = input_unspv\n",
    "    similarity_matrix_unspv = torch.einsum(\"ik,jk->ij\", input_unspv, input_unspv)\n",
    "    prob_target_unspv_pos = F.softmax(alpha * similarity_matrix_unspv, dim=1)\n",
    "    prob_target_unspv_neg = F.softmax(alpha * -similarity_matrix_unspv, dim=1)\n",
    "    \n",
    "    # 计算有监督相关性，以及log prob\n",
    "    similarity_matrix = torch.einsum(\"ik,jk->ij\", inputs_logit, inputs_logit)\n",
    "    log_prob_pos = torch.log(F.softmax(similarity_matrix, dim=1)+ eps)\n",
    "    log_prob_neg = torch.log(F.softmax(-similarity_matrix, dim=1)+ eps)\n",
    "\n",
    "    # 计算gate\n",
    "    gate_unspv_pos = torch.sign(torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - 0.75454967))\n",
    "    gate_unspv_neg = torch.sign(torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - (-0.85592163)))\n",
    "    gate_unspv_pos_label_correction = torch.sign(\n",
    "        torch.max(0.0 * similarity_matrix_unspv, similarity_matrix_unspv - 0.85592163))\n",
    "    gate_unspv_neg_label_correction = torch.sign(\n",
    "        torch.max(0.0 * similarity_matrix_unspv, -similarity_matrix_unspv - (-0.75454967)))\n",
    "\n",
    "    # 计算l2 loss\n",
    "    if with_l2reg:\n",
    "        reg = torch.mean(torch.sum(torch.pow(inputs_logit, 2), dim=1)).float()\n",
    "        l2loss = torch.mul(0.25 * 0.002, reg)\n",
    "    else:\n",
    "        l2loss = 0.0\n",
    "\n",
    "    #计算无监督loss\n",
    "    unspv_loss_pos = gen_loss_sub_unspv(log_prob_pos,prob_target_unspv_pos)  \n",
    "    unspv_loss_neg = gen_loss_sub_unspv(log_prob_neg,prob_target_unspv_neg) \n",
    "     \n",
    "    # 计算正样本的loss\n",
    "    xent_loss_pos_clean = gen_loss_sub(mask*gate_unspv_pos,log_prob_pos,bias=-1)\n",
    "    xent_loss_pos_noise = gen_loss_sub(mask*(1. - gate_unspv_pos),log_prob_pos,bias=0.)\n",
    "    xent_loss_pos_label_correction = gen_loss_sub(mask_neg*gate_unspv_pos_label_correction,log_prob_pos,bias=0.)\n",
    "      \n",
    "    xent_loss_pos_list = [xent_loss_pos_clean, xent_loss_pos_noise, xent_loss_pos_label_correction]\n",
    "    loss_grad_match = gen_meta_loss(xent_loss_pos_list,net)\n",
    "\n",
    "    xent_loss_pos = xent_loss_pos_clean + loss_grad_match * 0.001\n",
    "\n",
    "    ## 计算负样本的loss\n",
    "    xent_loss_neg_clean = gen_loss_sub(mask_neg*gate_unspv_neg,log_prob_neg,bias=0.)\n",
    "    xent_loss_neg_noise = gen_loss_sub(mask_neg*(1. - gate_unspv_neg),log_prob_neg,bias=0.)\n",
    "    xent_loss_neg_label_correction = gen_loss_sub(mask_pos*gate_unspv_neg_label_correction,log_prob_neg,bias=0.)\n",
    " \n",
    "    xent_loss_neg_list = [xent_loss_neg_clean, xent_loss_neg_noise, xent_loss_neg_label_correction]\n",
    "    loss_grad_match = gen_meta_loss(xent_loss_neg_list,net)\n",
    "\n",
    "    xent_loss_neg = xent_loss_neg_clean + loss_grad_match * 0.001\n",
    " \n",
    "    ## 计算最终的loss\n",
    "    xent_loss_neg_all = xent_loss_neg + 0.01 * unspv_loss_neg\n",
    "    xent_loss_pos_all = xent_loss_pos + 0.01 * unspv_loss_pos\n",
    "    xent_loss = xent_loss_neg_all + xent_loss_pos_all\n",
    "\n",
    "    loss = (l2loss + xent_loss)\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
